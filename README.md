Arquitetura para **Local (Ollama)**, o README precisa ser totalmente reescrito para refletir os novos prÃ©-requisitos (instalaÃ§Ã£o do Ollama, download de modelos) e a eliminaÃ§Ã£o da dependÃªncia da OpenAI.

Aqui estÃ¡ o **README.md** definitivo para a versÃ£o "Local LLM Edition" da sua POC.

---

# ğŸ¦™ E-commerce Intelligent Chatbot (Local LLM Edition)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![Ollama](https://img.shields.io/badge/Backend-Ollama_Local-orange)
![Llama 3](https://img.shields.io/badge/Model-Llama_3_8B-purple)
![Cost](https://img.shields.io/badge/Cost-Free-green)

## ğŸ“‹ VisÃ£o Geral Executiva

Esta Prova de Conceito (POC) demonstra uma soluÃ§Ã£o de **IA Generativa HÃ­brida** rodando 100% localmente. O projeto resolve o desafio de custos de API e privacidade de dados, utilizando modelos open-source (Llama 3) para atender clientes de e-commerce.

A arquitetura implementa um sistema de **Roteamento Inteligente** que diferencia:
1.  **Busca SemÃ¢ntica (RAG):** Para recomendaÃ§Ã£o e descriÃ§Ã£o de produtos (Dados NÃ£o-Estruturados).
2.  **AnÃ¡lise de Dados (Agentes):** Para consulta de status de pedidos e prazos (Dados Estruturados).

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

O sistema elimina a dependÃªncia de nuvem (OpenAI) substituindo-a pelo runtime local **Ollama**.

```mermaid
flowchart LR
    User[UsuÃ¡rio] --> UI[Streamlit UI]
    UI --> Router{Classificador}
    
    subgraph "Local Inference (Ollama)"
    Router -- "Produtos" --> RAG[RAG Pipeline]
    RAG --> Embed[nomic-embed-text]
    Embed --> VectorDB[(FAISS)]
    
    Router -- "Pedidos" --> Agent[Pandas Agent]
    Agent --> Llama3[LLM: Llama 3]
    end
    
    VectorDB --> Llama3
    Llama3 --> UI
```

---

## ğŸ’» PrÃ©-requisitos de Hardware

Como a IA roda na sua mÃ¡quina, recomenda-se:
*   **RAM:** MÃ­nimo 8GB (16GB Recomendado).
*   **Processador:** Recente (Intel i5/i7, AMD Ryzen ou Apple Silicon M1/M2/M3).
*   **EspaÃ§o em Disco:** ~6GB livres (para os modelos).

---

## ğŸš€ Guia de InstalaÃ§Ã£o Passo-a-Passo

### Passo 1: Configurar o Ollama (O "CÃ©rebro" Local)

1.  Baixe e instale o **Ollama** em [ollama.com](https://ollama.com).
2.  ApÃ³s instalar, abra seu terminal (PowerShell ou CMD) e baixe os modelos necessÃ¡rios:

    ```bash
    # Baixa o modelo de linguagem (LLM) - ~4.7GB
    ollama pull llama3

    # Baixa o modelo de embeddings (para o RAG) - Leve
    ollama pull nomic-embed-text
    ```
3.  **Verifique se estÃ¡ rodando:** Mantenha o aplicativo Ollama aberto (Ã­cone na barra de tarefas).

### Passo 2: Clonar e Preparar o Projeto

1.  Clone este repositÃ³rio:
    ```bash
    git clone https://github.com/seu-usuario/ecommerce-chatbot-local.git
    cd ecommerce-chatbot-local
    ```

2.  (Opcional) Crie um ambiente virtual:
    ```bash
    python -m venv venv
    # Windows:
    .\venv\Scripts\activate
    # Linux/Mac:
    source venv/bin/activate
    ```

### Passo 3: Instalar DependÃªncias Python

Instale as bibliotecas necessÃ¡rias (versÃµes fixadas para estabilidade):

```bash
pip install -r requirements.txt
```

### Passo 4: Executar a AplicaÃ§Ã£o

```bash
streamlit run app.py
```

---

## ğŸ§ª Como Testar

A interface abrirÃ¡ no seu navegador. Tente as seguintes interaÃ§Ãµes para testar os dois "lados" do cÃ©rebro da IA:

### 1. Teste de Produtos (Usa RAG + Vector Store)
*O modelo busca no catÃ¡logo por similaridade.*
*   *"Estou procurando um tÃªnis para correr maratonas."*
*   *"VocÃª tem algum fone que cancela barulho?"*
*   *"Me recomende uma cadeira para trabalhar o dia todo."*

### 2. Teste de Pedidos (Usa Agente + Pandas)
*O modelo lÃª a tabela, filtra dados e calcula datas.*
*   *"Qual o status do pedido 5005?"*
*   *"Quantos pedidos foram cancelados?"*
*   *"O pedido do cliente 3 jÃ¡ foi entregue?"*

---

## ğŸ“‚ Estrutura do CÃ³digo

```text
ecommerce-chatbot-local/
â”‚
â”œâ”€â”€ app.py                 # Interface Principal (Streamlit)
â”œâ”€â”€ requirements.txt       # Lista de dependÃªncias
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ data_generator.py  # Gera dados sintÃ©ticos (Produtos/Pedidos)
    â”œâ”€â”€ rag_engine.py      # ConfiguraÃ§Ã£o do RAG com OllamaEmbeddings
    â”œâ”€â”€ agent_engine.py    # ConfiguraÃ§Ã£o do Agente com ChatOllama
    â””â”€â”€ router.py          # LÃ³gica de decisÃ£o (If/Else Keywords)
```

---

## âš ï¸ Troubleshooting (Problemas Comuns)

**Erro: "Connection refused" ou falha ao conectar**
*   **Causa:** O Ollama nÃ£o estÃ¡ rodando.
*   **SoluÃ§Ã£o:** Abra o aplicativo Ollama no seu computador ou rode `ollama serve` no terminal.

**Erro: "Model not found"**
*   **Causa:** VocÃª esqueceu de baixar os modelos.
*   **SoluÃ§Ã£o:** Rode `ollama pull llama3` e `ollama pull nomic-embed-text`.

**LentidÃ£o na resposta**
*   **Causa:** Rodar LLMs localmente exige CPU/GPU.
*   **Obs:** A primeira pergunta pode demorar mais pois o sistema estÃ¡ carregando o modelo na memÃ³ria RAM.

---

### Chatbot POC

![X](img/01.png)

---

## ğŸ‘¨â€ğŸ’» Autor

**Kleber Augusto**
*Applied AI Engineer*

---
*Projeto desenvolvido como POC para demonstrar viabilidade de IA Generativa Local (Privacy-First).*